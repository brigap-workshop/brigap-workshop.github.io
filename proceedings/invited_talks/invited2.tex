Algorithms that produce textual output can sometimes ``hallucinate'', producing texts that express information that differs from what is required. 
In this presentation, I will talk about hallucination in Data-Text NLG, focusing on situations in which the task of the algorithm is to express a known body of information both fully and accurately. 
Various attempts have been made to clarify the notion of hallucination, and to distinguish between different types of hallucinations that can occur in the above-mentioned situations. 
I will examine some of these classifications and ask: 

(1) Are the existing classifications well defined? 
(2) How feasible in practice is it to apply these classifications to concrete cases of Data-Text NLG? (This is joint work with Eduardo Calo and Albert Gatt, both at Utrecht University.) 
(3) How useful are the distinctions that these classifications make, for example for determining the seriousness of a hallucination, or for redesigning the NLG algorithm so as to avoid hallucinations? 
And finally, if time permits (4) What does our investigation tell us about hallucinations in other NLG situations, for instance in Question-Answering? 
