- abstract: "We propose a Natural Language Inference (NLI) system based on compositional\
    \ semantics. \nThe system combines lightblue, a syntactic and semantic parser\
    \ grounded in Combinatory Categorial Grammar (CCG) and Dependent Type Semantics\
    \ (DTS), with wani, an automated theorem prover for Dependent Type Theory (DTT).\
    \ \nBecause each computational step reflects a theoretical assumption, system\
    \ evaluation serves as a form of hypothesis verification. \nWe evaluate the inference\
    \ system using the Japanese Semantic Test Suite JSeM, and demonstrate how error\
    \ analysis provides feedback to improve both the system and the underlying linguistic\
    \ theory."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@is.ocha.ac.jp'
    first_name: Asa
    homepage: https://morning85.github.io/
    last_name: Tomita
    name: Asa Tomita
    username: ~Asa_Tomita1
  - emails: matsubara.mai@is.ocha.ac.jp
    first_name: matsubara.mai@is.ocha.ac.jp
    institution: NA
    last_name: matsubara.mai@is.ocha.ac.jp
    name: matsubara.mai@is.ocha.ac.jp
    username: matsubara.mai@is.ocha.ac.jp
  - emails: daido.hinari@is.ocha.ac.jp
    first_name: daido.hinari@is.ocha.ac.jp
    institution: NA
    last_name: daido.hinari@is.ocha.ac.jp
    name: daido.hinari@is.ocha.ac.jp
    username: daido.hinari@is.ocha.ac.jp
  - dblp_id: https://dblp.org/pid/52/46
    emails: '****@is.ocha.ac.jp'
    first_name: Daisuke
    google_scholar_id: https://scholar.google.co.jp/citations?hl=ja&user=GWMfDLwAAAAJ
    homepage: https://daisukebekki.github.io/
    institution: Ochanomizu University
    last_name: Bekki
    name: Daisuke Bekki
    orcid: https://orcid.org/0000-0002-9988-1260
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Bekki/2520156
    username: ~Daisuke_Bekki1
  decision: BriGap-2
  file: 5.pdf
  id: 5
  openreview_id: TqFACH7vhL
  pdf_file: f058859796e08e9a50487e27886d398867c6f242.pdf
  title: Natural Language Inference with CCG Parser and Automated Theorem Prover for
    DTS
- abstract: 'Recent studies employing Large Language Models (LLMs) to test the Argument
    from the Poverty of the Stimulus (APS) have yielded contrasting results across
    syntactic phenomena. This paper investigates the hypothesis that characteristics
    of the stimuli used in recent studies, including lexical ambiguities and structural
    complexities, may confound model performance. A methodology is proposed for re-evaluating
    LLM competence on syntactic prediction, focusing on GPT-2. This involves: 1) establishing
    a baseline on previously used (both filtered and unfiltered) stimuli, and 2) generating
    a new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini
    2.5 Pro Preview) guided by linguistically-informed templates designed to mitigate
    identified confounds. Our preliminary findings indicate that GPT-2 demonstrates
    notably improved performance on these refined PG stimuli compared to baselines,
    suggesting that stimulus quality significantly influences outcomes in surprisal-based
    evaluations of LLM syntactic competency.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@aucklanduni.ac.nz'
    first_name: Timothy
    homepage: https://profiles.auckland.ac.nz/timothy-pistotti
    last_name: Pistotti
    name: Timothy Pistotti
    username: ~Timothy_Pistotti1
  - emails: '****@auckland.ac.nz'
    first_name: Jason
    last_name: Brown
    name: Jason Brown
    username: ~Jason_Brown1
  - dblp_id: https://dblp.org/pid/w/MichaelJWitbrock
    emails: '****@auckland.ac.nz'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ruQ7SNYAAAAJ
    institution: University of Auckland
    last_name: Witbrock
    middle_name: J.
    name: Michael J. Witbrock
    orcid: https://orcid.org/0000-0002-7554-0971
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Witbrock/2819135
    username: ~Michael_J._Witbrock1
  decision: BriGap-2
  file: 6.pdf
  id: 6
  openreview_id: NDNO1rUudx
  pdf_file: 606671cd2dd019595fccc954f6721aad4bfe63d8.pdf
  title: Evaluating The Impact of Stimulus Quality in Investigations of LLM Language
    Performance
- abstract: In the field of natural language processing, the construction of "linguistic
    pipelines", which draw on insights from theoretical linguistics, stands in a complementary
    relationship to the prevailing paradigm of large language models. The rapid development
    of these pipelines has been fueled by recent advancements, including the emergence
    of Dependent Type Semantics (DTS) — a type-theoretic framework for natural language
    semantics. While DTS has been successfully applied to analyze complex linguistic
    phenomena such as anaphora and presupposition, its capability to account for modal
    expressions remains an underexplored area. This study aims to address this gap
    by proposing a framework that extends DTS with modal types. This extension broadens
    the scope of linguistic phenomena that DTS can account for, including an analysis
    of modal subordination, where anaphora interacts with modal expressions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@is.ocha.ac.jp'
    first_name: Aoi
    homepage: https://sites.google.com/is.ocha.ac.jp/iimuraaoi
    last_name: Iimura
    name: Aoi Iimura
    username: ~Aoi_Iimura1
  - emails: '****@ocha.ac.jp'
    first_name: Teruyuki
    homepage: https://teruyuki-mizuno.github.io/
    institution: Ochanomizu University
    last_name: Mizuno
    name: Teruyuki Mizuno
    username: ~Teruyuki_Mizuno1
  - dblp_id: https://dblp.org/pid/52/46
    emails: '****@is.ocha.ac.jp'
    first_name: Daisuke
    google_scholar_id: https://scholar.google.co.jp/citations?hl=ja&user=GWMfDLwAAAAJ
    homepage: https://daisukebekki.github.io/
    institution: Ochanomizu University
    last_name: Bekki
    name: Daisuke Bekki
    orcid: https://orcid.org/0000-0002-9988-1260
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Bekki/2520156
    username: ~Daisuke_Bekki1
  decision: BriGap-2
  file: 7.pdf
  id: 7
  openreview_id: p0FBZXHpM8
  pdf_file: 3a6afb97635ac67993921a3ea29aaac9b80bebcd.pdf
  title: Modal Subordination in Dependent Type Semantics
- abstract: Recent studies probing the Argument from the Poverty of the Stimulus (APS)
    have applied Large Language Models (LLMs) to test the learnability of complex
    syntax through surprisal-based metrics. However, divergent conclusions raise questions
    concerning the insights these metrics offer. While Wilcox et al. (2024) used direct
    minimal pair comparisons (the "wh-effect") to demonstrate that models successfully
    generalise knowledge of filler-gap dependencies, Lan et al. (2024) used a Difference-in-Differences
    (DiD) metric and found that models largely fail on parasitic gaps (PGs). This
    paper argues that the direct minimal pair approach offers greater diagnostic transparency.
    We demonstrate this by generating a full 8-permutation paradigm of refined PG
    stimuli and evaluating the GPT-2 model used in previous studies with a systematic
    Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across all
    four tested conditions, indicating robust knowledge of filler-gap licensing principles
    even in complex PG environments. This finding, which contrasts with the more ambiguous
    results from DiD-style metrics, suggests that the choice of evaluation metric
    is critical for assessing an LLM's syntactic competence.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@aucklanduni.ac.nz'
    first_name: Timothy
    homepage: https://profiles.auckland.ac.nz/timothy-pistotti
    last_name: Pistotti
    name: Timothy Pistotti
    username: ~Timothy_Pistotti1
  - emails: '****@auckland.ac.nz'
    first_name: Jason
    last_name: Brown
    name: Jason Brown
    username: ~Jason_Brown1
  - dblp_id: https://dblp.org/pid/w/MichaelJWitbrock
    emails: '****@auckland.ac.nz'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ruQ7SNYAAAAJ
    institution: University of Auckland
    last_name: Witbrock
    middle_name: J.
    name: Michael J. Witbrock
    orcid: https://orcid.org/0000-0002-7554-0971
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Witbrock/2819135
    username: ~Michael_J._Witbrock1
  decision: BriGap-2
  file: 8.pdf
  id: 8
  openreview_id: dCNsetcfPE
  pdf_file: f89d274a2b9292ba42412935b5f982e02ab3be89.pdf
  title: 'Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic
    Assessments'
- abstract: This paper presents our work in progress on devising a developmentally
    inspired, syntax-based curriculum for small-scale language model training. While
    developmental approaches to curriculum learning (CL) in Natural Language Processing
    (NLP) have been explored before, prior work has been limited in corpora analysis
    and curriculum quantification. We present the most comprehensive effort to date
    to analyze child-directed speech through syntactic patterns, as the former studies
    focused on either specific constructions or age groups. Our current contributions
    include a toolkit for organizing syntactically annotated datasets into 13 syntactic
    categories, and a dataset of six corpora reorganized according to a developmental
    curriculum. Our future work will focus on investigating the effect of this curriculum
    on language modeling performance.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@itu.dk'
    first_name: Arzu
    homepage: https://pure.itu.dk/en/persons/arzu-burcu-güven
    last_name: Güven
    middle_name: Burcu
    name: Arzu Burcu Güven
    orcid: https://orcid.org/0009-0008-3905-2840
    username: ~Arzu_Burcu_Güven1
  - dblp_id: https://dblp.org/pid/184/8526
    emails: '****@live.nl'
    first_name: Rob
    google_scholar_id: https://scholar.google.com/citations?user=lU4zpOEAAAAJ&hl=nl
    homepage: https://robvanderg.github.io/
    institution: IT University of Copenhagen
    last_name: Goot
    middle_name: Van Der
    name: Rob van der Goot
    semantic_scholar_id: https://www.semanticscholar.org/author/Rob-van-der-Goot/3449407
    username: ~Rob_van_der_Goot1
  - dblp_id: https://dblp.org/pid/203/9462
    emails: '****@itu.dk'
    first_name: Anna
    google_scholar_id: https://scholar.google.com/citations?user=5oCYOE0AAAAJ&hl=en
    homepage: https://annargrs.github.io
    institution: IT University of Copenhagen
    last_name: Rogers
    name: Anna Rogers
    orcid: https://orcid.org/0000-0002-4845-4023
    semantic_scholar_id: https://www.semanticscholar.org/author/Anna-Rogers/145046059
    username: ~Anna_Rogers1
  decision: BriGap-2
  file: 13.pdf
  id: 13
  openreview_id: Jl6G3EHB6n
  pdf_file: 22637d7ed0cb0076370051a056c0e5564da0aa1a.pdf
  title: Towards Developmentally Motivated Curriculum Learning for Language Models
- abstract: The aim of this paper is to present a case study of a fruitful and, hopefully,
    inspiring interaction between formal and computational linguistics. A variety
    of NLP tools and resources have been used in linguistic investigations of the
    symmetry of coordination, leading to novel theoretical arguments. The converse
    impact of theoretical results on NLP work has been successful only in some cases.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/34/6295
    emails: '****@ipipan.waw.pl'
    first_name: Adam
    google_scholar_id: https://scholar.google.pl/citations?hl=en&user=volQDFMAAAAJ
    homepage: http://zil.ipipan.waw.pl/AdamPrzepiorkowski
    institution: University of Warsaw and Polish Academy of Sciences
    last_name: Przepiórkowski
    name: Adam Przepiórkowski
    orcid: https://orcid.org/0000-0002-4398-2636
    username: ~Adam_Przepiórkowski1
  - dblp_id: https://dblp.org/pid/126/8872
    emails: '****@gmail.com'
    first_name: Agnieszka
    google_scholar_id: https://scholar.google.com/citations?user=z2lIvyEAAAAJ
    homepage: http://zil.ipipan.waw.pl/AgnieszkaPatejuk
    institution: Institute of Computer Science, Polish Academy of Sciences
    last_name: Patejuk
    name: Agnieszka Patejuk
    orcid: https://orcid.org/0000-0002-2367-9170
    username: ~Agnieszka_Patejuk1
  decision: BriGap-2
  file: 14.pdf
  id: 14
  openreview_id: X70wChoRYz
  pdf_file: 9bd430b798f181d2bedae6115312b4cdcfd8fd5c.pdf
  title: Coordination of Theoretical and Computational Linguistics
- abstract: This paper presents a computational resource for exploring semantic parsing
    and reasoning through a strictly formal lense. Inspired by the framework of Lexical
    Functional Grammar, our system allows for modular exploration of different aspects
    of semantic parsing. It consists of a hand-coded formal grammar combining syntactic
    and semantic annotations, producing basic semantic representations. The system
    provides the option to extend these basic semantics via rewrite rules in a principled
    fashion to explore more complex reasoning. The result is a layered system enabling
    an incremental approach to semantic parsing. We illustrate this approach with
    examples from the Fracas testsuite demonstrating its overall functionality and
    viability.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@uni-konstanz.de'
    first_name: Mark-Matthias
    homepage: https://ling.sprachwiss.uni-konstanz.de/pages/home/zymla/
    last_name: Zymla
    name: Mark-Matthias Zymla
    username: ~Mark-Matthias_Zymla1
  - emails: '****@uni-konstanz.de'
    first_name: Kascha
    institution: Universität Konstanz
    last_name: Kruschwitz
    name: Kascha Kruschwitz
    username: ~Kascha_Kruschwitz1
  - emails: '****@uni-konstanz.de'
    first_name: Paul
    last_name: Zodl
    name: Paul Zodl
    orcid: https://orcid.org/0009-0005-4815-1611
    username: ~Paul_Zodl1
  decision: BriGap-2
  file: 15.pdf
  id: 15
  openreview_id: FcGgw7q1v0
  pdf_file: f0869603b994cbca6d1283764053f79f8badc8cd.pdf
  title: An instructive implementation of semantic parsing and reasoning using Lexical
    Functional Grammar
- abstract: The correlation between reading times and surprisal is well known in psycholinguistics
    and is easy to observe. There is also a correlation between reading times and
    structural integration, which is, however, harder to detect (Gibson, 2000). This
    correlation has been studied using parsing models whose outputs are linked to
    reading times. In this paper, we study the relevance of memory-based effects in
    reading times and how to predict them using neural language models. We find that
    integration costs significantly improve surprisal-based reading time prediction.
    Inspired by Timkey and Linzen (2023), we design a small-scale autoregressive transformer
    language model in which attention heads are supervised by dependency relations.
    We compare this model to a standard variant by checking how well each model’s
    outputs correlate with human reading times and find that predicted attention scores
    can be effectively used as proxies for syntactic integration costs to predict
    self-paced reading times.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@uni-duesseldorf.de'
    first_name: Lukas
    google_scholar_id: https://scholar.google.com/citations?user=187ImWwAAAAJ&hl=en
    homepage: https://lukasmielczarek.de
    institution: Heinrich-Heine Universität Düsseldorf and Heinrich-Heine Universität
      Düsseldorf
    last_name: Mielczarek
    name: Lukas Mielczarek
    username: ~Lukas_Mielczarek1
  - dblp_id: https://dblp.org/pid/185/5507
    emails: timothee.bernard@ens-lyon.org
    first_name: Timothée
    google_scholar_id: https://scholar.google.fr/citations?user=S3dtSYQAAAAJ
    homepage: https://vantot.gitlab.io/website/
    institution: Université Paris Cité
    last_name: Bernard
    name: Timothée Bernard
    orcid: https://orcid.org/0000-0003-4172-6986
    username: ~Timothée_Bernard1
  - dblp_id: https://dblp.org/pid/25/5562
    emails: '****@phil.hhu.de'
    first_name: Laura
    google_scholar_id: https://scholar.google.de/citations?user=gmFgdBwAAAAJ&hl=de&oi=ao
    homepage: https://user.phil.hhu.de/kallmeyer/
    institution: Heinrich Heine University Düsseldorf, Germany
    last_name: Kallmeyer
    name: Laura Kallmeyer
    orcid: https://orcid.org/0000-0001-9691-5990
    semantic_scholar_id: https://www.semanticscholar.org/author/Laura-Kallmeyer/2692018
    username: ~Laura_Kallmeyer1
  - emails: '****@hhu.de'
    first_name: Katharina
    homepage: https://www.ling.hhu.de/psycho-neuro/spalek
    last_name: Spalek
    name: Katharina Spalek
    orcid: https://orcid.org/0000-0001-7641-7310
    username: ~Katharina_Spalek1
  - dblp_id: https://dblp.org/pid/95/6.html
    emails: '****@u-paris.fr'
    first_name: Benoit
    google_scholar_id: https://scholar.google.com/citations?user=9vyYVd0AAAAJ&hl=en
    homepage: http://www.linguist.univ-paris-diderot.fr/~bcrabbe/
    institution: Université de Paris
    last_name: Crabbé
    name: Benoit Crabbé
    username: ~Benoit_Crabbé1
  decision: BriGap-2
  file: 17.pdf
  id: 17
  openreview_id: lHenWviTfs
  pdf_file: 788c1c26d6e87fc9c0a90c4eda0f25782c0f0a10.pdf
  title: Modelling Expectation-based and Memory-based Predictors of Human Reading
    Times with Syntax-guided Attention
- abstract: 'Large Language Models (LLMs) demonstrate remarkable linguistic capabilities
    but lack explicit syntactic knowledge grounded in formal grammatical theory. This
    paper introduces a syntax-guided parameter-efficient fine-tuning approach that
    integrates formal syntactic constraints into transformer-based models using Low-Rank
    Adaptation (LoRA). We develop a hybrid training objective incorporating violations
    of syntactic well-formedness derived from dependency parsing and context-free
    grammar constraints. Our method is evaluated on established syntactic benchmarks
    including BLiMP, CoLA, and SyntaxGym targeting specific grammatical phenomena.
    Results show consistent improvements in syntactic competence: 7.3\% average improvement
    on BLiMP overall, with particularly strong gains of 9.5\% on agreement phenomena
    and filler-gap dependencies, alongside 5.8\% improvement on CoLA MCC scores, while
    maintaining performance on general NLP tasks. The parameter-efficient approach
    reduces training time by 77\% compared to full fine-tuning while achieving substantial
    syntactic gains. This work demonstrates a practical pathway for incorporating
    linguistic theory into modern NLP systems, yielding more interpretable and grammatically
    robust language models.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@gmail.com'
    first_name: Prasanth
    google_scholar_id: https://scholar.google.com/citations?view_op=list_works&hl=en&user=saNs5hEAAAAJ&gmla=AFix5MaMd2md5pK3wbzalZB96rdnm9BiwOfZpRSZi8mh4c6GsUJ2f5IrWcbZSKDwi3a7t7zlfcdyAs-SsigzI93EktSPGdC7DtAmLhPzaaM&sciund=15188482945741840010
    institution: R V College of Engineering
    last_name: Prasanth
    name: Prasanth
    username: ~Prasanth1
  decision: BriGap-2
  file: 18.pdf
  id: 18
  openreview_id: DYxMvM2Tjf
  pdf_file: c895530665161ad0b1ae346955a4012eb09ccd09.pdf
  title: 'Syntax-Guided Parameter Efficient Fine-Tuning: Integrating Formal Grammatical
    Constraints into Language Models'
- abstract: We investigate the impact of center embedding and selectional restrictions
    on neural latent tree models' tendency to induce self-embedding structures. To
    this aim we compare their behavior in different controlled artificial environments
    involving noun phrases modified by relative clauses, with different quantity of
    available training data. Our results provide evidence that the existence of multiple
    center self-embedding is a stronger incentive than selectional restrictions alone,
    but that the combination of both is the best incentive overall. We also show that
    different architectures benefit very differently from these incentives.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@umontreal.ca'
    first_name: Antoine
    google_scholar_id: https://scholar.google.com/citations?user=qt1y9C8AAAAJ&hl=en
    institution: Université de Montréal
    last_name: Venant
    name: Antoine Venant
    username: ~Antoine_Venant1
  - emails: '****@umontreal.ca'
    first_name: Yutaka
    institution: Université de Montréal
    last_name: Suzuki
    name: Yutaka Suzuki
    username: ~Yutaka_Suzuki1
  decision: BriGap-2
  file: 21.pdf
  id: 21
  openreview_id: IYd0sJ4TZu
  pdf_file: 742b3e6a7066c84a40dbcee22c281df2994ccb8d.pdf
  title: On the relative impact of categorical and semantic information on the induction
    of self-embedding structures
- abstract: Human communications make frequent use of plural predications. Plural
    sentences have been observed to be highly ambiguous. There are many theoretical
    and experimental studies on the nature and logic of plurality in linguistics and
    philosophy. In this paper, we investigate two lexical aspects of predicates which
    influence the resolution of plural ambiguity from the novel perspective of the
    predictions of large language models (LLMs), in particular, BERT and GPT-2. The
    results of our models differ from the results gained from human experiments. While
    human language users have certain bias in their interpretation of plural sentences,
    the biases are not prominent in the language models we looked at. By the end of
    the paper, we discuss some potential implications of the results.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@umass.edu'
    first_name: Jia
    homepage: https://umassonline.academia.edu/JiaRen
    last_name: Ren
    name: Jia Ren
    username: ~Jia_Ren1
  decision: BriGap-2
  file: 22.pdf
  id: 22
  openreview_id: TwIEnaivM8
  pdf_file: a45f393bc3d05699a1e86e4a936c919f4571cdd4.pdf
  title: Plural Ambiguity in Language Models
