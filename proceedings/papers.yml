- abstract: "We propose a Natural Language Inference (NLI) system based on compositional\
    \ semantics. \nThe system combines lightblue, a syntactic and semantic parser\
    \ grounded in Combinatory Categorial Grammar (CCG) and Dependent Type Semantics\
    \ (DTS), with wani, an automated theorem prover for Dependent Type Theory (DTT).\
    \ \nBecause each computational step reflects a theoretical assumption, system\
    \ evaluation serves as a form of hypothesis verification. \nWe evaluate the inference\
    \ system using the Japanese Semantic Test Suite JSeM, and demonstrate how error\
    \ analysis provides feedback to improve both the system and the underlying linguistic\
    \ theory."
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@is.ocha.ac.jp'
    first_name: Asa
    homepage: https://morning85.github.io/
    last_name: Tomita
    name: Asa Tomita
    username: ~Asa_Tomita1
  - emails: '****@is.ocha.ac.jp'
    first_name: Mai
    institution: Ochanomizu University
    last_name: Matsubara
    name: Mai Matsubara
  - emails: '****@is.ocha.ac.jp'
    first_name: Hinari
    institution: Ochanomizu University
    last_name: Daido
    name: Hinari Daido
  - dblp_id: https://dblp.org/pid/52/46
    emails: '****@is.ocha.ac.jp'
    first_name: Daisuke
    google_scholar_id: https://scholar.google.co.jp/citations?hl=ja&user=GWMfDLwAAAAJ
    homepage: https://daisukebekki.github.io/
    institution: Ochanomizu University
    last_name: Bekki
    name: Daisuke Bekki
    orcid: https://orcid.org/0000-0002-9988-1260
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Bekki/2520156
    username: ~Daisuke_Bekki1
  decision: BriGap-2
  file: 5.pdf
  id: 5
  openreview_id: TqFACH7vhL
  pdf_file: f058859796e08e9a50487e27886d398867c6f242.pdf
  title: Natural Language Inference with CCG Parser and Automated Theorem Prover for
    DTS
- abstract: 'Recent studies employing Large Language Models (LLMs) to test the Argument
    from the Poverty of the Stimulus (APS) have yielded contrasting results across
    syntactic phenomena. This paper investigates the hypothesis that characteristics
    of the stimuli used in recent studies, including lexical ambiguities and structural
    complexities, may confound model performance. A methodology is proposed for re-evaluating
    LLM competence on syntactic prediction, focusing on GPT-2. This involves: 1) establishing
    a baseline on previously used (both filtered and unfiltered) stimuli, and 2) generating
    a new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini
    2.5 Pro Preview) guided by linguistically-informed templates designed to mitigate
    identified confounds. Our preliminary findings indicate that GPT-2 demonstrates
    notably improved performance on these refined PG stimuli compared to baselines,
    suggesting that stimulus quality significantly influences outcomes in surprisal-based
    evaluations of LLM syntactic competency.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@aucklanduni.ac.nz'
    first_name: Timothy
    homepage: https://profiles.auckland.ac.nz/timothy-pistotti
    last_name: Pistotti
    name: Timothy Pistotti
    username: ~Timothy_Pistotti1
  - emails: '****@auckland.ac.nz'
    first_name: Jason
    last_name: Brown
    name: Jason Brown
    username: ~Jason_Brown1
  - dblp_id: https://dblp.org/pid/w/MichaelJWitbrock
    emails: '****@auckland.ac.nz'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ruQ7SNYAAAAJ
    institution: University of Auckland
    last_name: Witbrock
    middle_name: J.
    name: Michael J. Witbrock
    orcid: https://orcid.org/0000-0002-7554-0971
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Witbrock/2819135
    username: ~Michael_J._Witbrock1
  decision: BriGap-2
  file: 6.pdf
  id: 6
  openreview_id: NDNO1rUudx
  pdf_file: 606671cd2dd019595fccc954f6721aad4bfe63d8.pdf
  title: Evaluating The Impact of Stimulus Quality in Investigations of LLM Language
    Performance
- abstract: In the field of natural language processing, the construction of "linguistic
    pipelines", which draw on insights from theoretical linguistics, stands in a complementary
    relationship to the prevailing paradigm of large language models. The rapid development
    of these pipelines has been fueled by recent advancements, including the emergence
    of Dependent Type Semantics (DTS) — a type-theoretic framework for natural language
    semantics. While DTS has been successfully applied to analyze complex linguistic
    phenomena such as anaphora and presupposition, its capability to account for modal
    expressions remains an underexplored area. This study aims to address this gap
    by proposing a framework that extends DTS with modal types. This extension broadens
    the scope of linguistic phenomena that DTS can account for, including an analysis
    of modal subordination, where anaphora interacts with modal expressions.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@is.ocha.ac.jp'
    first_name: Aoi
    homepage: https://sites.google.com/is.ocha.ac.jp/iimuraaoi
    last_name: Iimura
    name: Aoi Iimura
    username: ~Aoi_Iimura1
  - emails: '****@ocha.ac.jp'
    first_name: Teruyuki
    homepage: https://teruyuki-mizuno.github.io/
    institution: Ochanomizu University
    last_name: Mizuno
    name: Teruyuki Mizuno
    username: ~Teruyuki_Mizuno1
  - dblp_id: https://dblp.org/pid/52/46
    emails: '****@is.ocha.ac.jp'
    first_name: Daisuke
    google_scholar_id: https://scholar.google.co.jp/citations?hl=ja&user=GWMfDLwAAAAJ
    homepage: https://daisukebekki.github.io/
    institution: Ochanomizu University
    last_name: Bekki
    name: Daisuke Bekki
    orcid: https://orcid.org/0000-0002-9988-1260
    semantic_scholar_id: https://www.semanticscholar.org/author/D.-Bekki/2520156
    username: ~Daisuke_Bekki1
  decision: BriGap-2
  file: 7.pdf
  id: 7
  openreview_id: p0FBZXHpM8
  pdf_file: 3a6afb97635ac67993921a3ea29aaac9b80bebcd.pdf
  title: Modal Subordination in Dependent Type Semantics
- abstract: Recent studies probing the Argument from the Poverty of the Stimulus (APS)
    have applied Large Language Models (LLMs) to test the learnability of complex
    syntax through surprisal-based metrics. However, divergent conclusions raise questions
    concerning the insights these metrics offer. While Wilcox et al. (2024) used direct
    minimal pair comparisons (the "wh-effect") to demonstrate that models successfully
    generalise knowledge of filler-gap dependencies, Lan et al. (2024) used a Difference-in-Differences
    (DiD) metric and found that models largely fail on parasitic gaps (PGs). This
    paper argues that the direct minimal pair approach offers greater diagnostic transparency.
    We demonstrate this by generating a full 8-permutation paradigm of refined PG
    stimuli and evaluating the GPT-2 model used in previous studies with a systematic
    Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across all
    four tested conditions, indicating robust knowledge of filler-gap licensing principles
    even in complex PG environments. This finding, which contrasts with the more ambiguous
    results from DiD-style metrics, suggests that the choice of evaluation metric
    is critical for assessing an LLM's syntactic competence.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@aucklanduni.ac.nz'
    first_name: Timothy
    homepage: https://profiles.auckland.ac.nz/timothy-pistotti
    last_name: Pistotti
    name: Timothy Pistotti
    username: ~Timothy_Pistotti1
  - emails: '****@auckland.ac.nz'
    first_name: Jason
    last_name: Brown
    name: Jason Brown
    username: ~Jason_Brown1
  - dblp_id: https://dblp.org/pid/w/MichaelJWitbrock
    emails: '****@auckland.ac.nz'
    first_name: Michael
    google_scholar_id: https://scholar.google.com/citations?hl=en&user=ruQ7SNYAAAAJ
    institution: University of Auckland
    last_name: Witbrock
    middle_name: J.
    name: Michael J. Witbrock
    orcid: https://orcid.org/0000-0002-7554-0971
    semantic_scholar_id: https://www.semanticscholar.org/author/M.-Witbrock/2819135
    username: ~Michael_J._Witbrock1
  decision: BriGap-2
  file: 8.pdf
  id: 8
  openreview_id: dCNsetcfPE
  pdf_file: f89d274a2b9292ba42412935b5f982e02ab3be89.pdf
  title: 'Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic
    Assessments'
- abstract: The aim of this paper is to present a case study of a fruitful and, hopefully,
    inspiring interaction between formal and computational linguistics. A variety
    of NLP tools and resources have been used in linguistic investigations of the
    symmetry of coordination, leading to novel theoretical arguments. The converse
    impact of theoretical results on NLP work has been successful only in some cases.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/34/6295
    emails: '****@ipipan.waw.pl'
    first_name: Adam
    google_scholar_id: https://scholar.google.pl/citations?hl=en&user=volQDFMAAAAJ
    homepage: http://zil.ipipan.waw.pl/AdamPrzepiorkowski
    institution: University of Warsaw and Polish Academy of Sciences
    last_name: Przepiórkowski
    name: Adam Przepiórkowski
    orcid: https://orcid.org/0000-0002-4398-2636
    username: ~Adam_Przepiórkowski1
  - dblp_id: https://dblp.org/pid/126/8872
    emails: '****@gmail.com'
    first_name: Agnieszka
    google_scholar_id: https://scholar.google.com/citations?user=z2lIvyEAAAAJ
    homepage: http://zil.ipipan.waw.pl/AgnieszkaPatejuk
    institution: Institute of Computer Science, Polish Academy of Sciences
    last_name: Patejuk
    name: Agnieszka Patejuk
    orcid: https://orcid.org/0000-0002-2367-9170
    username: ~Agnieszka_Patejuk1
  decision: BriGap-2
  file: 14.pdf
  id: 14
  openreview_id: X70wChoRYz
  pdf_file: 7cbf9a9f3bdef6f0eb05d4e894aa96a71f726139.pdf
  title: Coordination of Theoretical and Computational Linguistics
- abstract: This paper presents a computational resource for exploring semantic parsing
    and reasoning through a strictly formal lense. Inspired by the framework of Lexical
    Functional Grammar, our system allows for modular exploration of different aspects
    of semantic parsing. It consists of a hand-coded formal grammar combining syntactic
    and semantic annotations, producing basic semantic representations. The system
    provides the option to extend these basic semantics via rewrite rules in a principled
    fashion to explore more complex reasoning. The result is a layered system enabling
    an incremental approach to semantic parsing. We illustrate this approach with
    examples from the Fracas testsuite demonstrating its overall functionality and
    viability.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@uni-konstanz.de'
    first_name: Mark-Matthias
    homepage: https://ling.sprachwiss.uni-konstanz.de/pages/home/zymla/
    last_name: Zymla
    name: Mark-Matthias Zymla
    username: ~Mark-Matthias_Zymla1
  - emails: '****@uni-konstanz.de'
    first_name: Kascha
    institution: Universität Konstanz
    last_name: Kruschwitz
    name: Kascha Kruschwitz
    username: ~Kascha_Kruschwitz1
  - emails: '****@uni-konstanz.de'
    first_name: Paul
    last_name: Zodl
    name: Paul Zodl
    orcid: https://orcid.org/0009-0005-4815-1611
    username: ~Paul_Zodl1
  decision: BriGap-2
  file: 15.pdf
  id: 15
  openreview_id: FcGgw7q1v0
  pdf_file: f0869603b994cbca6d1283764053f79f8badc8cd.pdf
  title: An instructive implementation of semantic parsing and reasoning using Lexical
    Functional Grammar
- abstract: The correlation between reading times and surprisal is well known in psycholinguistics
    and is easy to observe. There is also a correlation between reading times and
    structural integration, which is, however, harder to detect (Gibson, 2000). This
    correlation has been studied using parsing models whose outputs are linked to
    reading times. In this paper, we study the relevance of memory-based effects in
    reading times and how to predict them using neural language models. We find that
    integration costs significantly improve surprisal-based reading time prediction.
    Inspired by Timkey and Linzen (2023), we design a small-scale autoregressive transformer
    language model in which attention heads are supervised by dependency relations.
    We compare this model to a standard variant by checking how well each model’s
    outputs correlate with human reading times and find that predicted attention scores
    can be effectively used as proxies for syntactic integration costs to predict
    self-paced reading times.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@uni-duesseldorf.de'
    first_name: Lukas
    google_scholar_id: https://scholar.google.com/citations?user=187ImWwAAAAJ&hl=en
    homepage: https://lukasmielczarek.de
    institution: Heinrich-Heine Universität Düsseldorf and Heinrich-Heine Universität
      Düsseldorf
    last_name: Mielczarek
    name: Lukas Mielczarek
    username: ~Lukas_Mielczarek1
  - dblp_id: https://dblp.org/pid/185/5507
    emails: '****@ens-lyon.org'
    first_name: Timothée
    google_scholar_id: https://scholar.google.fr/citations?user=S3dtSYQAAAAJ
    homepage: https://vantot.gitlab.io/website/
    institution: Université Paris Cité
    last_name: Bernard
    name: Timothée Bernard
    orcid: https://orcid.org/0000-0003-4172-6986
    username: ~Timothée_Bernard1
  - dblp_id: https://dblp.org/pid/25/5562
    emails: '****@phil.hhu.de'
    first_name: Laura
    google_scholar_id: https://scholar.google.de/citations?user=gmFgdBwAAAAJ&hl=de&oi=ao
    homepage: https://user.phil.hhu.de/kallmeyer/
    institution: Heinrich Heine University Düsseldorf, Germany
    last_name: Kallmeyer
    name: Laura Kallmeyer
    orcid: https://orcid.org/0000-0001-9691-5990
    semantic_scholar_id: https://www.semanticscholar.org/author/Laura-Kallmeyer/2692018
    username: ~Laura_Kallmeyer1
  - emails: '****@hhu.de'
    first_name: Katharina
    homepage: https://www.ling.hhu.de/psycho-neuro/spalek
    last_name: Spalek
    name: Katharina Spalek
    orcid: https://orcid.org/0000-0001-7641-7310
    username: ~Katharina_Spalek1
  - dblp_id: https://dblp.org/pid/95/6.html
    emails: '****@u-paris.fr'
    first_name: Benoit
    google_scholar_id: https://scholar.google.com/citations?user=9vyYVd0AAAAJ&hl=en
    homepage: http://www.linguist.univ-paris-diderot.fr/~bcrabbe/
    institution: Université de Paris
    last_name: Crabbé
    name: Benoit Crabbé
    username: ~Benoit_Crabbé1
  decision: BriGap-2
  file: 17.pdf
  id: 17
  openreview_id: lHenWviTfs
  pdf_file: 788c1c26d6e87fc9c0a90c4eda0f25782c0f0a10.pdf
  title: Modelling Expectation-based and Memory-based Predictors of Human Reading
    Times with Syntax-guided Attention
- abstract: We investigate the impact of center embedding and selectional restrictions
    on neural latent tree models' tendency to induce self-embedding structures. To
    this aim we compare their behavior in different controlled artificial environments
    involving noun phrases modified by relative clauses, with different quantity of
    available training data. Our results provide evidence that the existence of multiple
    center self-embedding is a stronger incentive than selectional restrictions alone,
    but that the combination of both is the best incentive overall. We also show that
    different architectures benefit very differently from these incentives.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@umontreal.ca'
    first_name: Antoine
    google_scholar_id: https://scholar.google.com/citations?user=qt1y9C8AAAAJ&hl=en
    institution: Université de Montréal
    last_name: Venant
    name: Antoine Venant
    username: ~Antoine_Venant1
  - emails: '****@umontreal.ca'
    first_name: Yutaka
    institution: Université de Montréal
    last_name: Suzuki
    name: Yutaka Suzuki
    username: ~Yutaka_Suzuki1
  decision: BriGap-2
  file: 21.pdf
  id: 21
  openreview_id: IYd0sJ4TZu
  pdf_file: 742b3e6a7066c84a40dbcee22c281df2994ccb8d.pdf
  title: On the relative impact of categorical and semantic information on the induction
    of self-embedding structures
- abstract: 'Human communication routinely relies on plural predication, and plural
    sentences are often ambiguous (see, e.g., Scha, 1984; Dalrymple et al., 1998a,
    to name a few). Building on extensive theoretical and experimental work in linguistics
    and philosophy, we ask whether large language models (LLMs) exhibit the same interpretive
    biases that humans show when resolving plural ambiguity. We focus on two lexical
    factors: (i) the collective bias of certain predicates (e.g., size/shape adjectives)
    and (ii) the symmetry bias of predicates. To probe these tendencies, we apply
    two complementary methods to premise–hypothesis pairs: an embedding-based heuristic
    using OpenAI’s text-embedding-3-large/small (OpenAI, 2024, 2025) with cosine similarity,
    and supervised NLI models (bart-large-mnli, roberta-large-mnli) (Lewis et al.,
    2020; Liu et al., 2019; Williams et al., 2018a; Facebook AI, 2024b,a) that yield
    asymmetric, calibrated entailment probabilities. Results show partial sensitivity
    to predicate-level distinctions, but neither method reproduces the robust human
    pattern, where neutral predicates favor entailment and strongly non-symmetric
    predicates disfavor it. These findings highlight both the potential and the limits
    of current LLMs: as cognitive models, they fall short of capturing human-like
    interpretive biases; as engineering systems, their representations of plural semantics
    remain unstable for tasks requiring precise entailment.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@umass.edu'
    first_name: Jia
    homepage: https://umassonline.academia.edu/JiaRen
    last_name: Ren
    name: Jia Ren
    username: ~Jia_Ren1
  decision: BriGap-2
  file: 22.pdf
  id: 22
  openreview_id: TwIEnaivM8
  pdf_file: 086ffb321668477a4f5ad237a22860a12da571cb.pdf
  title: 'Plural Interpretive Biases: A Comparison Between Human Language Processing
    and Language Models'
